{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Using cached psycopg2-2.9.3-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'e:\\Ian\\Virtual Environments\\env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2\n",
    "import os\n",
    "import scrapy\n",
    "from scrapy.utils.project import get_project_settings\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.loader import ItemLoader\n",
    "from twisted.internet import reactor\n",
    "\n",
    "%run E:\\Ian\\Ebay_Project\\card_scraper\\card_scraper\\items.py\n",
    "\n",
    "import re\n",
    "# Reactor restart\n",
    "from crochet import setup, wait_for\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "setup()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from webscrapingapi import WebScrapingApiClient\n",
    "\n",
    "# client = WebScrapingApiClient(api_key='YOUR_API_KEY')\n",
    "\n",
    "# response = client.get('https://webscrapingapi.com/', params ={\n",
    "#     # API Parameters\n",
    "#     # Set to 0 (off, default) or 1 (on) depending on whether or not to render JavaScript on the target web page. JavaScript rendering is done by using a browser.\n",
    "#     'render_js': 1,\n",
    "#     # Set datacenter (default) or residential depending on whether proxy type you want to use for your scraping request. Please note that a single residential proxy API request is counted as 25 API requests.\n",
    "#     'proxy_type': 'datacenter',\n",
    "#     # Specify the 2-letter code of the country you would like to use as a proxy geolocation for your scraping API request. Supported countries differ by proxy type, please refer to the Proxy Locations section for details.\n",
    "#     'country': 'us',\n",
    "#     # Set depending on whether or not to use the same proxy address to your request.\n",
    "#     'session': 1,\n",
    "#     # Specify the maximum timeout in milliseconds you would like to use for your scraping API request. In order to force a timeout, you can specify a number such as 1000. This will abort the request after 1000ms and return whatever HTML response was obtained until this point in time.\n",
    "#     'timeout': 10000,\n",
    "#     # Set desktop (default) or mobile or tablet, depending on whether the device type you want to your for your scraping request.\n",
    "#     'device': 'desktop',\n",
    "#     # Specify the option you would like to us as conditional for your scraping API request. Can only be used when the parameter render_js=1 is activated.\n",
    "#     'wait_until': 'domcontentloaded',\n",
    "#     # Some websites may use javascript frameworks that may require a few extra seconds to load their content. This parameters specifies the time in miliseconds to wait for the website. Recommended values are in the interval 5000-10000.\n",
    "#     'wait_for': 0,\n",
    "# }, headers={\n",
    "#     # API Headers\n",
    "#     'authorization': 'bearer test',\n",
    "#     # Specify custom cookies to be passed to the request.\n",
    "#     'cookie': 'test_cookie=abc; cookie_2=def'\n",
    "# })\n",
    "\n",
    "# print(response.status_code)\n",
    "# print(response.headers);\n",
    "# print(response.content);\n",
    "\n",
    "\"\"\" \n",
    "import http.client\n",
    "\n",
    "conn = http.client.HTTPSConnection(\"api.webscrapingapi.com\")\n",
    "\n",
    "conn.request(\"GET\", \"/v1?api_key=snejc2HGikGwZvrKMLJx5FTz42wOAmHZ&url=https%3A%2F%2Fapi.ipify.org%2F%3Fformat%3Djson&device=desktop&proxy_type=datacenter&session=500\")\n",
    "\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "\n",
    "proxy = data.decode(\"utf-8\") \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ip\":\"2.56.103.151\"}\n",
      "2.56.103.151\n"
     ]
    }
   ],
   "source": [
    "\"\"\" print(proxy)\n",
    "proxy = proxy.replace(\"{\\\"ip\\\":\\\"\", \"\")\n",
    "proxy = proxy.replace(\"\\\"}\", \"\")\n",
    "print(proxy)\n",
    "proxy = \"http://\"+proxy\n",
    "os.environ['http_proxy'] = proxy  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "translate = {'青眼の白龍':'Blue-Eyes White Dragon',\n",
    "             'ブルーアイズ・ホワイト・ドラゴン':'Blue-Eyes White Dragon',\n",
    "             '遊★戯★王':'Yu-Gi-Oh!','ユーギオオー!':'Yu-Gi-Oh!',\n",
    "             '遊戯王':'Yu-Gi-Oh!'}\n",
    "\n",
    "class 青眼の白龍_SM_51_sold_spider(scrapy.Spider):\n",
    "    name = 'BEWD'\n",
    "    allowed_domains = ['auctions.yahoo.co.jp']\n",
    "    start_urls =['https://auctions.yahoo.co.jp/closedsearch/closedsearch?va=%E9%9D%92%E7%9C%BC%E3%81%AE%E7%99%BD%E9%BE%8D&b=1&n=50&auccat=&tab_ex=commerce&ei=utf-8&aq=-1&oq=&sc_i=&exflg=&p=%E9%9D%92%E7%9C%BC%E3%81%AE%E7%99%BD%E9%BE%8D+sm-51&x=0&y=0','https://auctions.yahoo.co.jp/closedsearch/closedsearch?va=%E9%9D%92%E7%9C%BC%E3%81%AE%E7%99%BD%E9%BE%8D+lb&b=1&n=50&select=3&auccat=&tab_ex=commerce&ei=utf-8&aq=-1&oq=&sc_i=&exflg=&p=%E9%9D%92%E7%9C%BC%E3%81%AE%E7%99%BD%E9%BE%8D+lb-01&x=0&y=0']\n",
    "    custom_settings = {'DOWNLOAD_DELAY':2}\n",
    "    #scrape links of pages to scrape\n",
    "    def parse(self,response):\n",
    "        #location of item page link for all items on page\n",
    "        all_links = response.xpath('//h3//a/@href').extract()\n",
    "        for link in all_links:\n",
    "            yield scrapy.Request(link,callback = self.item_parse)\n",
    "        # Yields next page of search to scrape\n",
    "        next_page = response.xpath('//ul[@class=\"Pager__lists\"]//li//a[@data-ylk = \"rsec:pagination;slk:next;pos:1\"]/@href')\n",
    "        yield scrapy.Request(next_page,self.parse)\n",
    "    def item_parse(self,response):\n",
    "        #Call Item Loader to parse the responses\n",
    "        l = ItemLoader(item=CardScraperItem(), response=response)\n",
    "        l.add_xpath('auction_id','//*[@id=\"l-main\"]//div[@class = \"l-right\"]//li[6]//dd/text()')\n",
    "        l.add_xpath('title','//*[@id=\"ProductTitle\"]/div/h1//text()')    \n",
    "        l.add_xpath('price','//*[@id=\"l-sub\"]//dd[@class = \"Price__value\"]/text()')\n",
    "        l.add_xpath('bids','//dd[@class = \"Count__number\"]/text()')\n",
    "        l.add_xpath('tax','//*[@id=\"l-sub\"]//dd[@class = \"Price__value\"]/span/text()')\n",
    "        l.add_xpath('auction_start','//*[@id=\"l-main\"]//div[@class = \"l-left\"]//li[2]//dd/text()')\n",
    "        l.add_xpath('auction_end','//*[@id=\"l-main\"]//div[@class = \"l-left\"]//li[3]//dd/text()')\n",
    "        l.add_xpath('auction_extension','//*[@id=\"l-main\"]//div[@class = \"l-left\"]//li[4]//dd/text()')\n",
    "        l.add_xpath('best_offer_accepted','//*[@id=\"l-main\"]//div[@class = \"l-left\"]//li[5]//dd/text()')\n",
    "        l.add_xpath('image_list','//div[contains(@class,\"ProductImage__body\")]//img/@src')\n",
    "        return l.load_item()\n",
    "    \n",
    "    \n",
    "# process = CrawlerProcess(settings={'FEEDS':{'items.csv': {'format':'csv'}}})\n",
    "\n",
    "\n",
    "\n",
    "# process = CrawlerProcess(get_project_settings())\n",
    "# process.crawl(青眼の白龍_SM_51_sold_spider)\n",
    "# process.start()\n",
    "# !scrapy crawl BEWD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twisted.internet import reactor\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from scrapy.utils.log import configure_logging\n",
    "\n",
    "# Enable logging for CrawlerRunner\n",
    "\n",
    "configure_logging()\n",
    "\n",
    "runner = CrawlerRunner(settings=get_project_settings())\n",
    "\n",
    "runner.crawl(青眼の白龍_SM_51_sold_spider)\n",
    "\n",
    "runner.addBoth(lambda _: reactor.stop())\n",
    "\n",
    "reactor.run() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_spider' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Ian\\Ebay_Project\\card_scraper\\card_scraper\\spiders\\Yahoo_to_SQL.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Ian/Ebay_Project/card_scraper/card_scraper/spiders/Yahoo_to_SQL.ipynb#ch0000002?line=0'>1</a>\u001b[0m run_spider()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run_spider' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" run_spider() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '青眼の白龍_SM_51_sold_spider' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Ian\\Ebay_Project\\card_scraper\\card_scraper\\spiders\\Yahoo_to_SQL.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Ian/Ebay_Project/card_scraper/card_scraper/spiders/Yahoo_to_SQL.ipynb#ch0000004?line=30'>31</a>\u001b[0m \u001b[39m# Enable logging when using CrawlerRunner\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Ian/Ebay_Project/card_scraper/card_scraper/spiders/Yahoo_to_SQL.ipynb#ch0000004?line=31'>32</a>\u001b[0m configure_logging()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Ian/Ebay_Project/card_scraper/card_scraper/spiders/Yahoo_to_SQL.ipynb#ch0000004?line=33'>34</a>\u001b[0m run_spider(青眼の白龍_SM_51_sold_spider)\n",
      "\u001b[1;31mNameError\u001b[0m: name '青眼の白龍_SM_51_sold_spider' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" from multiprocessing import Process\n",
    "\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from scrapy.utils.log import configure_logging\n",
    "# from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "\n",
    "# from myprojectscraper.spiders.my_homepage_spider import MyHomepageSpider\n",
    "# from myprojectscraper.spiders.my_spider import MySpider\n",
    "\n",
    "from twisted.internet import reactor\n",
    "\n",
    "# Create Process around the CrawlerRunner\n",
    "class CrawlerRunnerProcess(Process):\n",
    "    def __init__(self, spider):\n",
    "        Process.__init__(self)\n",
    "        self.runner = CrawlerRunner(get_project_settings())\n",
    "        self.spider = 青眼の白龍_SM_51_sold_spider\n",
    "\n",
    "    def run(self):\n",
    "        deferred = self.runner.crawl(self.spider)\n",
    "        deferred.addBoth(lambda _: reactor.stop())\n",
    "        reactor.run(installSignalHandlers=False)\n",
    "\n",
    "# The wrapper to make it run multiple spiders, multiple times\n",
    "def run_spider(spider):\n",
    "    crawler = CrawlerRunnerProcess(spider)\n",
    "    crawler.start()\n",
    "    crawler.join()\n",
    "\n",
    "# Enable logging when using CrawlerRunner\n",
    "configure_logging()\n",
    "\n",
    "run_spider(青眼の白龍_SM_51_sold_spider) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8679fc6cd90eb79c346670f8d398a15728623de227f0b67125cda0a71a51d63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
